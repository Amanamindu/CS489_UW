{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOU_a2 (v1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q1: Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\\begin{align}\n",
    "\\frac{d \\sigma(z)}{dz}\n",
    "    &= -\\frac{-e^{-z}}{{(1+e^{-z})}^2}\\\\\n",
    "    &= \\frac{1}{1+e^{-z}} \\cdot \\frac{e^{-z}}{1+e^{-z}}\\\\\n",
    "    &= \\sigma(z)(1-\\sigma(z))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To help you with $\\LaTeX$, and to show you my expectations, here is a sample taken from the lecture notes, taken from the 3rd and 4th page of the notes entitled \"Error Backpropagation\". It has nothing to do with the solution to this question, but just demonstrates some of the features of $\\LaTeX$. Notice how I include English statments to guide the reader through the derivation.\n",
    "\n",
    "<a target=_new href=\"http://detexify.kirelabs.org/classify.html\">This web page</a> is very handy for identifying $\\LaTeX$ symbols.\n",
    "\n",
    "---\n",
    "More generally, for $\\vec{x} \\in \\mathbb{R}^X$, $\\vec{h} \\in \\mathbb{R}^H$, and $\\vec{y} \\in \\mathbb{R}^Y$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial \\alpha_i}\n",
    "  &= \\frac{d h_i}{d \\alpha_i} \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "  \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right] \\cdot\n",
    "  \\left[ \\frac{\\partial E}{\\partial \\beta_1} \\ \\cdots \\ \\frac{\\partial E}{\\partial \\beta_Y} \\right] \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "   \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right]\n",
    "   \\left[ \\begin{array}{c}\n",
    "     \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "     \\vdots \\\\\n",
    "     \\frac{\\partial E}{\\partial \\beta_Y} \\end{array} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "Thus, for all elements,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\alpha_H}\n",
    "\\end{array} \\right] &=\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{d h_1}{d \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{d h_H}{d \\alpha_H}\n",
    "\\end{array} \\right]\n",
    "\\odot\n",
    "\\left[ \\begin{array}{ccc}\n",
    "  M_{11} & \\cdots & M_{Y1} \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  M_{1H} & \\cdots & M_{YH}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\beta_Y}\n",
    "\\end{array} \\right] \\\\\n",
    "%\n",
    "\\frac{\\partial E}{\\partial \\vec{\\alpha}} &=\n",
    "\\frac{d \\vec{h}}{d \\vec{\\alpha}} \\odot M^\\mathrm{T}\n",
    "\\frac{\\partial E}{\\partial \\vec{\\beta}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q2: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let $e_k = e^{z_k}$. \n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_j}\n",
    "    &= \\frac{\\partial E}{\\partial e_j} \\cdot \\frac{d e_j}{d z_j}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "For $\\frac{d e_j}{d z_j}$, we have:\n",
    "$$\\begin{align}\n",
    "\\frac{d e_j}{d z_j} = e^{z_j}\n",
    "\\end{align}\n",
    "$$\n",
    "For $\\frac{\\partial E}{\\partial e_j}$, we have:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E}{\\partial e_j}\n",
    "    &= \\sum\\limits_{i=1}^k \\frac{\\partial E}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial e_j}\n",
    "\\end{align}\n",
    "$$\n",
    "For $\\frac{\\partial E}{\\partial y_i}$, we have:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E}{\\partial y_i}\n",
    "    &= - \\frac{t_i}{y_i}\n",
    "\\end{align}\n",
    "$$\n",
    "Let $g = \\sum\\limits_{j=1}^k e^{z_j} = \\sum\\limits_{j=1}^k e_j$.\n",
    "  \n",
    "For $\\frac{\\partial y_i}{\\partial e_j}$, we have:\n",
    "  \n",
    "If $i \\neq j$:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial y_i}{\\partial e_j} = - \\frac{e_i}{g^2}\n",
    "\\end{align}\n",
    "$$\n",
    "if $i = j$:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial y_i}{\\partial e_j} = \\frac{1}{g} - \\frac{e_i}{g^2}\n",
    "\\end{align}\n",
    "$$\n",
    "Therefore, we have:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_j}\n",
    "    &= \\frac{\\partial E}{\\partial e_j} \\cdot \\frac{d e_j}{d z_j}\\\\\n",
    "    &= e^{z_j} \\sum\\limits_{i=1}^k \\frac{\\partial E}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial e_j}\\\\\n",
    "    &= e^{z_j} \\sum\\limits_{i=1}^k - \\frac{t_i}{y_i} \\cdot \\frac{\\partial y_i}{\\partial e_j}\\\\\n",
    "    &= e^{z_j} \\sum\\limits_{i=1}^k \\frac{t_i}{y_i}\\frac{e_i}{g^2} - e^{z_j}\\frac{t_j}{y_j}\\frac{1}{g} \\\\\n",
    "    &= e^{z_j} (- \\frac{t_j}{y_j}\\frac{1}{g} + \\sum\\limits_{i=1}^k \\frac{t_i}{y_i}\\frac{e_i}{g^2} )\n",
    "\\end{align}\n",
    "$$\n",
    "where $g = \\sum\\limits_{j=1}^k e^{z_j} = \\sum\\limits_{j=1}^k e_j$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q3: Top-Layer Error Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "(a)Cross entropy, and logistic activation function.\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z} \n",
    "    &= \\frac{\\partial E}{\\partial y} \\frac{dy}{dz}\\\\\n",
    "    &= (\\frac{-t}{y} + \\frac{1-t}{1-y}) \\cdot y(1-y)\\\\\n",
    "    &= -t(1-y) + (1-t)y\n",
    "\\end{align}\n",
    "$$\n",
    "  \n",
    "(b)Mean squared error, and identity activation function, $\\sigma(z) = z$.\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z} \n",
    "    &= \\frac{\\partial E}{\\partial y} \\frac{dy}{dz}\\\\\n",
    "    &= 2(y-t) \\cdot 1\\\\\n",
    "    &= 2(y-t)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q4: Implementing Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Supplied Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     16,
     35
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Supplied functions\n",
    "\n",
    "def NSamples(x):\n",
    "    '''\n",
    "        n = NSamples(x)\n",
    "        \n",
    "        Returns the number of samples in a batch of inputs.\n",
    "        \n",
    "        Input:\n",
    "         x   is a 2D array\n",
    "        \n",
    "        Output:\n",
    "         n   is an integer\n",
    "    '''\n",
    "    return len(x)\n",
    "\n",
    "def Shuffle(inputs, targets):\n",
    "    '''\n",
    "        s_inputs, s_targets = Shuffle(inputs, targets)\n",
    "        \n",
    "        Randomly shuffles the dataset.\n",
    "        \n",
    "        Inputs:\n",
    "         inputs     array of inputs\n",
    "         targets    array of corresponding targets\n",
    "         \n",
    "        Outputs:\n",
    "         s_inputs   shuffled array of inputs\n",
    "         s_targets  corresponding shuffled array of targets\n",
    "    '''\n",
    "    data = list(zip(inputs,targets))\n",
    "    np.random.shuffle(data)\n",
    "    s_inputs, s_targets = zip(*data)\n",
    "    return np.array(s_inputs), np.array(s_targets)\n",
    "\n",
    "def Logistic(z):\n",
    "    '''\n",
    "        y = Logistic(z)\n",
    "\n",
    "        Applies the logistic function to each element in z.\n",
    "\n",
    "        Input:\n",
    "         z    is a scalar, list or array\n",
    "\n",
    "        Output:\n",
    "         y    is the same shape as z\n",
    "    '''\n",
    "    return 1. / (1 + np.exp(-z) )\n",
    "\n",
    "def Logistic_p(h):\n",
    "    '''\n",
    "        yp = Logistic_p(h)\n",
    "        \n",
    "        Returns the slope of the logistic function at z when h = Logistic(z).\n",
    "        Note the h is the input, NOT z.\n",
    "    '''\n",
    "    return h*(1.-h)\n",
    "\n",
    "def Identity(z):\n",
    "    '''\n",
    "        y = Identity(z)\n",
    "\n",
    "        Does nothing... simply returns z.\n",
    "\n",
    "        Input:\n",
    "         z    is a scalar, list or array\n",
    "\n",
    "        Output:\n",
    "         y    is the same shape as z\n",
    "    '''\n",
    "    return z\n",
    "\n",
    "def Identity_p(h):\n",
    "    '''\n",
    "        yp = Identity_p(h)\n",
    "        \n",
    "        Returns the slope of the identity function h.\n",
    "    '''\n",
    "    return np.ones_like(h)\n",
    "\n",
    "def OneHot(z):\n",
    "    '''\n",
    "        y = OneHot(z)\n",
    "\n",
    "        Applies the one-hot function to the vectors in z.\n",
    "        Example:\n",
    "          OneHot([[0.9, 0.1], [-0.5, 0.1]])\n",
    "          returns np.array([[1,0],[0,1]])\n",
    "\n",
    "        Input:\n",
    "         z    is a 2D array of samples\n",
    "\n",
    "        Output:\n",
    "         y    is an array the same shape as z\n",
    "    '''\n",
    "    y = []\n",
    "    # Locate the max of each row\n",
    "    for zz in z:\n",
    "        idx = np.argmax(zz)\n",
    "        b = np.zeros_like(zz)\n",
    "        b[idx] = 1.\n",
    "        y.append(b)\n",
    "    y = np.array(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    def __init__(self, n_nodes, act='logistic'):\n",
    "        '''\n",
    "            lyr = Layer(n_nodes, act='logistic')\n",
    "            \n",
    "            Creates a layer object.\n",
    "            \n",
    "            Inputs:\n",
    "             n_nodes  the number of nodes in the layer\n",
    "             act      specifies the activation function\n",
    "                      Use 'logistic' or 'identity'\n",
    "        '''\n",
    "        self.N = n_nodes  # number of nodes in this layer\n",
    "        self.h = []       # node activities\n",
    "        self.b = np.zeros(self.N)  # biases\n",
    "        \n",
    "        # Activation functions\n",
    "        self.sigma = Logistic\n",
    "        self.sigma_p = (lambda : Logistic_p(self.h))\n",
    "        if act=='identity':\n",
    "            self.sigma = Identity\n",
    "            self.sigma_p = (lambda : Identity_p(self.h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     2,
     65,
     81
    ]
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    def __init__(self, sizes, type='classifier'):\n",
    "        '''\n",
    "            net = Network(sizes, type='classifier')\n",
    "\n",
    "            Creates a Network and saves it in the variable 'net'.\n",
    "\n",
    "            Inputs:\n",
    "              sizes is a list of integers specifying the number\n",
    "                  of nodes in each layer\n",
    "                  eg. [5, 20, 3] will create a 3-layer network\n",
    "                      with 5 input, 20 hidden, and 3 output nodes\n",
    "              type can be either 'classifier' or 'regression', and\n",
    "                  sets the activation function on the output layer,\n",
    "                  as well as the loss function.\n",
    "                  'classifier': logistic, cross entropy\n",
    "                  'regression': linear, mean squared error\n",
    "        '''\n",
    "        self.n_layers = len(sizes)\n",
    "        self.lyr = []    # a list of Layers\n",
    "        self.W = []      # Weight matrices, indexed by the layer below it\n",
    "        \n",
    "        # Two common types of networks\n",
    "        # The member variable self.Loss refers to one of the implemented\n",
    "        # loss functions: MSE, or CrossEntropy.\n",
    "        # Call it using self.Loss(t)\n",
    "        if type=='classifier':\n",
    "            self.classifier = True\n",
    "            self.Loss = self.CrossEntropy\n",
    "            activation = 'logistic'\n",
    "        else:\n",
    "            self.classifier = False\n",
    "            self.Loss = self.MSE\n",
    "            activation = 'identity'\n",
    "\n",
    "        # Create and add Layers (using logistic for hidden layers)\n",
    "        for n in sizes[:-1]:\n",
    "            self.lyr.append( Layer(n) )\n",
    "   \n",
    "        # For the top layer, we use the appropriate activtaion function\n",
    "        self.lyr.append( Layer(sizes[-1], act=activation) )\n",
    "    \n",
    "        # Randomly initialize weight matrices\n",
    "        for idx in range(self.n_layers-1):\n",
    "            m = self.lyr[idx].N\n",
    "            n = self.lyr[idx+1].N\n",
    "            temp = np.random.normal(size=[m,n])/np.sqrt(m)\n",
    "            self.W.append(temp)\n",
    "\n",
    "\n",
    "    def FeedForward(self, x):\n",
    "        '''\n",
    "            y = net.FeedForward(x)\n",
    "\n",
    "            Runs the network forward, starting with x as input.\n",
    "            Returns the activity of the output layer.\n",
    "        '''\n",
    "        x = np.array(x)  # Convert input to array, in case it's not\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        z = x\n",
    "        self.lyr[0].h = x\n",
    "        for i in range(self.n_layers - 1):\n",
    "            z = self.lyr[i].h\n",
    "            layer = self.lyr[i+1]\n",
    "            w = self.W[i]\n",
    "            b = layer.b\n",
    "            sigma = layer.sigma\n",
    "            layer.h = sigma(np.dot(z, w) + b)\n",
    "            \n",
    "            \n",
    "        return self.lyr[-1].h\n",
    "\n",
    "    \n",
    "    def Evaluate(self, inputs, targets):\n",
    "        '''\n",
    "            E = net.Evaluate(data)\n",
    "\n",
    "            Computes the average loss over the supplied dataset.\n",
    "\n",
    "            Inputs\n",
    "             inputs  is an array of inputs\n",
    "             targets is a list of corresponding targets\n",
    "\n",
    "            Outputs\n",
    "             E is a scalar, the average loss\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        return self.Loss(targets)\n",
    "\n",
    "    def ClassificationAccuracy(self, inputs, targets):\n",
    "        '''\n",
    "            a = net.ClassificationAccuracy(data)\n",
    "            \n",
    "            Returns the fraction (between 0 and 1) of correct one-hot classifications\n",
    "            in the dataset.\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        yb = OneHot(y)\n",
    "        n_incorrect = np.sum(yb!=targets) / 2.\n",
    "        return 1. - float(n_incorrect) / NSamples(inputs)\n",
    "\n",
    "    \n",
    "    def CrossEntropy(self, t):\n",
    "        '''\n",
    "            E = net.CrossEntropy(t)\n",
    "\n",
    "            Evaluates the mean cross entropy loss between t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs:\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs:\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        y = self.lyr[-1].h\n",
    "        a = np.log(y)\n",
    "        b = np.log(1-y)\n",
    "        loss = np.mean(-t * a - (1 - t) * b)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def MSE(self, t):\n",
    "        '''\n",
    "            E = net.MSE(t)\n",
    "\n",
    "            Evaluates the MSE loss function using t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs:\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs:\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        y = self.lyr[-1].h\n",
    "        loss = np.mean(np.square(y - t))\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def BackProp(self, t, lrate=0.05):\n",
    "        '''\n",
    "            net.BackProp(targets, lrate=0.05)\n",
    "            \n",
    "            Given the current network state and targets t, updates the connection\n",
    "            weights and biases using the backpropagation algorithm.\n",
    "            \n",
    "            Inputs:\n",
    "             t      an array of targets (number of samples must match the\n",
    "                    network's output)\n",
    "             lrate  learning rate\n",
    "        '''\n",
    "        t = np.array(t)  # convert t to an array, in case it's not\n",
    "        n = self.n_layers\n",
    "        #===== YOUR CODE HERE =====\n",
    "        '''\n",
    "        if self.classifier == True:\n",
    "            y = self.lyr[-1].h\n",
    "            b1 = self.lyr[-1].b\n",
    "            W1 = self.W[-1]\n",
    "            x_hat = self.lyr[-2].h\n",
    "            x = self.lyr[-3].h\n",
    "            N = self.lyr[0].h.shape[0]\n",
    "\n",
    "            # calculate the last layer\n",
    "            dydZ1 = Logistic_p(y)\n",
    "            \n",
    "            # dydZ1 = Identity_p(y)\n",
    "            a = -t / y\n",
    "            b = (1-t) / (1-y)\n",
    "            dEdy = 1.0 / N * (a + b)\n",
    "            #print(dEdy)\n",
    "            # dEdy = y-t\n",
    "            dEdZ1 = dydZ1 * dEdy\n",
    "            dZ1dW1 = x_hat.T\n",
    "            dEdW1 = np.dot(dZ1dW1, dEdZ1)\n",
    "            dEdb1 = np.mean(dEdZ1, axis=0).T\n",
    "            \n",
    "            # update W and b\n",
    "            self.W[-1] -= lrate * dEdW1\n",
    "            self.lyr[-1].b = self.lyr[-1].b - lrate * dEdb1 \n",
    "            \n",
    "            # going backwards\n",
    "            dx_hatdZ0 = Logistic_p(x_hat)\n",
    "            dZ1dx_hat = W1.T\n",
    "            dEdx_hat = np.dot(dEdZ1, dZ1dx_hat)\n",
    "            dEdZ0 = dx_hatdZ0 * dEdx_hat\n",
    "            dZ0dW0 = x.T\n",
    "            dEdW0 = np.dot(dZ0dW0, dEdZ0)\n",
    "            dEdb0 = np.mean(dEdZ0, axis=0).T\n",
    "            \n",
    "            # update W and b\n",
    "            self.W[0] -= lrate * dEdW0\n",
    "            self.lyr[1].b = self.lyr[1].b - lrate * dEdb0\n",
    "            \n",
    "            # Lastly, change dEdZ1\n",
    "            #dEdZ1 = dEdZ0\n",
    "        else:\n",
    "            y = self.lyr[-1].h\n",
    "            b1 = self.lyr[-1].b\n",
    "            W1 = self.W[-1]\n",
    "            x_hat = self.lyr[-2].h\n",
    "            x = self.lyr[-3].h\n",
    "            N = self.lyr[0].h.shape[0]\n",
    "            \n",
    "            # calculate the last layer\n",
    "            dydZ1 = Identity_p(y)\n",
    "            dEdy = 1.0 / N * (y-t)\n",
    "            dEdZ1 = dydZ1 * dEdy\n",
    "            dZ1dW1 = x_hat.T\n",
    "            dEdW1 = np.dot(dZ1dW1, dEdZ1)\n",
    "            dEdb1 = np.mean(dEdZ1, axis=0).T\n",
    "            \n",
    "            # update W and b\n",
    "            self.W[-1] -= lrate * dEdW1\n",
    "            self.lyr[-1].b = self.lyr[-1].b - lrate * dEdb1 \n",
    "            \n",
    "            # going backwards\n",
    "            dx_hatdZ0 = Logistic_p(x_hat)\n",
    "            dZ1dx_hat = W1.T\n",
    "            dEdx_hat = np.dot(dEdZ1, dZ1dx_hat)\n",
    "            dEdZ0 = dx_hatdZ0 * dEdx_hat\n",
    "            dZ0dW0 = x.T\n",
    "            dEdW0 = np.dot(dZ0dW0, dEdZ0)\n",
    "            dEdb0 = np.mean(dEdZ0, axis=0).T\n",
    "            \n",
    "            # update W and b\n",
    "            self.W[0] -= lrate * dEdW0\n",
    "            self.lyr[1].b = self.lyr[1].b - lrate * dEdb0\n",
    "            \n",
    "        '''\n",
    "        y = self.lyr[-1].h\n",
    "        b1 = self.lyr[-1].b\n",
    "        W1 = self.W[-1]\n",
    "        x_hat = self.lyr[-2].h\n",
    "        N = self.lyr[0].h.shape[0]\n",
    "        \n",
    "        if self.classifier == True: \n",
    "            # calculate the last layer\n",
    "            dydZ1 = Logistic_p(y)      \n",
    "            # dydZ1 = Identity_p(y)\n",
    "            a = -t / y\n",
    "            b = (1-t) / (1-y)\n",
    "            dEdy = 1.0 / N * (a + b)\n",
    "            # dEdy = 1.0 / N * (y-t)\n",
    "            dEdZ1 = dydZ1 * dEdy\n",
    "            dZ1dW1 = x_hat.T\n",
    "            dEdW1 = np.dot(dZ1dW1, dEdZ1)\n",
    "            dEdb1 = np.mean(dEdZ1, axis=0).T\n",
    "            \n",
    "            # update W and b\n",
    "            self.W[-1] -= lrate * dEdW1\n",
    "            self.lyr[-1].b =  self.lyr[-1].b - lrate * dEdb1 \n",
    "            \n",
    "            for i in range(2, n):\n",
    "                # going backwards\n",
    "                layer = self.lyr[n-i]\n",
    "                previous_layer = self.lyr[n-i-1]\n",
    "                x_hat = layer.h\n",
    "                x = previous_layer.h\n",
    "                b0 = layer.b\n",
    "                W0 = self.W[n-i-1]\n",
    "                W1 = self.W[n-i]\n",
    "                dx_hatdZ0 = Logistic_p(x_hat)\n",
    "                dZ1dx_hat = W1.T\n",
    "                dEdx_hat = np.dot(dEdZ1, dZ1dx_hat)\n",
    "                dEdZ0 = dx_hatdZ0 * dEdx_hat\n",
    "                dZ0dW0 = x.T\n",
    "                dEdW0 = np.dot(dZ0dW0, dEdZ0)\n",
    "                dEdb0 = np.mean(dEdZ0, axis=0).T\n",
    "                # update W and b\n",
    "\n",
    "                W0 -= lrate * dEdW0\n",
    "                layer.b -= lrate * dEdb0\n",
    "                # Lastly, change dEdZ1\n",
    "                dEdZ1 = dEdZ0.copy()\n",
    "                \n",
    "        else:\n",
    "            # calculate the last layer\n",
    "            # dydZ1 = Logistic_p(y)\n",
    "            dydZ1 = Identity_p(y)\n",
    "            dEdy = 1.0 / N * (y-t)\n",
    "            dEdZ1 = dydZ1 * dEdy\n",
    "            dZ1dW1 = x_hat.T\n",
    "            dEdW1 = np.dot(dZ1dW1, dEdZ1)\n",
    "            dEdb1 = np.mean(dEdZ1, axis=0).T\n",
    "            # update W and b\n",
    "            self.W[-1] -= lrate * dEdW1\n",
    "            self.lyr[-1].b = self.lyr[-1].b - lrate * dEdb1  \n",
    "            \n",
    "            for i in range(2, n):\n",
    "                # going backwards\n",
    "                layer = self.lyr[n-i]\n",
    "                previous_layer = self.lyr[n-i-1]\n",
    "                x_hat = layer.h\n",
    "                x = previous_layer.h\n",
    "                b0 = layer.b\n",
    "                W0 = self.W[n-i-1]\n",
    "                W1 = self.W[n-i]\n",
    "                dx_hatdZ0 = Logistic_p(x_hat)\n",
    "                dZ1dx_hat = W1.T\n",
    "                dEdx_hat = np.dot(dEdZ1, dZ1dx_hat)\n",
    "                dEdZ0 = dx_hatdZ0 * dEdx_hat\n",
    "                dZ0dW0 = x.T\n",
    "                dEdW0 = np.dot(dZ0dW0, dEdZ0)\n",
    "                dEdb0 = np.mean(dEdZ0, axis=0).T\n",
    "                # update W and b\n",
    "                W0 -= lrate * dEdW0\n",
    "                layer.b -= lrate * dEdb0\n",
    "                \n",
    "                # Lastly, change dEdZ1\n",
    "                dEdZ1 = dEdZ0.copy()\n",
    "            \n",
    "                \n",
    "    def Learn(self, inputs, targets, lrate=0.05, epochs=1):\n",
    "        '''\n",
    "            Network.Learn(inputs, targets, lrate=0.05, epochs=1)\n",
    "\n",
    "            Run through the dataset 'epochs' number of times, incrementing the\n",
    "            network weights for each training sample. For each epoch, it\n",
    "            shuffles the order of the samples.\n",
    "\n",
    "            Inputs:\n",
    "              inputs  is an array of input samples\n",
    "              targets is a corresponding array of targets\n",
    "              lrate   is the learning rate (try 0.001 to 0.5)\n",
    "              epochs  is the number of times to go through the training data\n",
    "        '''\n",
    "        \n",
    "        #===== YOUR CODE HERE ====#\n",
    "        for epoch in range(epochs):\n",
    "            x, t = Shuffle(inputs, targets)\n",
    "            self.FeedForward(x)\n",
    "            self.BackProp(t, lrate)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 5 Classes in 8-Dimensional Space\n",
    "np.random.seed(15)\n",
    "noise = 0.1\n",
    "InputClasses = np.array([[1,0,1,0,0,1,1,0],\n",
    "                         [0,1,0,1,0,1,0,1],\n",
    "                         [0,1,1,0,1,0,0,1],\n",
    "                         [1,0,0,0,1,0,1,1],\n",
    "                         [1,0,0,1,0,1,0,1]], dtype=float)\n",
    "OutputClasses = np.array([[1,0,0,0,0],\n",
    "                          [0,1,0,0,0],\n",
    "                          [0,0,1,0,0],\n",
    "                          [0,0,0,1,0],\n",
    "                          [0,0,0,0,1]], dtype=float)\n",
    "n_input = np.shape(InputClasses)[1]\n",
    "n_output = np.shape(OutputClasses)[1]\n",
    "n_classes = np.shape(InputClasses)[0]\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 100\n",
    "training_output = []\n",
    "training_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    training_input.append(x)\n",
    "    training_output.append(t)\n",
    "\n",
    "# Create a test dataset\n",
    "n_samples = 100\n",
    "test_output = []\n",
    "test_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    test_input.append(x)\n",
    "    test_output.append(t)\n",
    "\n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a Network\n",
    "net = Network([n_input, 18, n_output], type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 0.7234102650666892\n",
      "     Accuracy = 26.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=500, lrate=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "Cross Entropy = 0.003651153826952998\n",
      "     Accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Training Set')\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set\n",
      "Cross Entropy = 0.003936938065971942\n",
      "     Accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Test Set')\n",
    "CE = net.Evaluate(test[0], test[1])\n",
    "accuracy = net.ClassificationAccuracy(test[0], test[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## You can also try using the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import Network_solutions\n",
    "importlib.reload(Network_solutions)\n",
    "net2 = Network_solutions.Network([n_input, 18, n_output], type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "Cross Entropy = 0.8874576713656722\n",
      "     Accuracy = 96.0%\n",
      "Test Set\n",
      "Cross Entropy = 0.9454835621238554\n",
      "     Accuracy = 92.0%\n"
     ]
    }
   ],
   "source": [
    "net2.Learn(train[0], train[1], epochs=500, lrate=1.)\n",
    "print('Training Set')\n",
    "CE = net2.Evaluate(train[0], train[1])\n",
    "accuracy = net2.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "print('Test Set')\n",
    "CE = net2.Evaluate(test[0], test[1])\n",
    "accuracy = net2.ClassificationAccuracy(test[0], test[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1D -> 1D (linear mapping)\n",
    "np.random.seed(846)\n",
    "n_input = 1\n",
    "n_output = 1\n",
    "slope = np.random.rand() - 0.5\n",
    "intercept = np.random.rand()*2. - 1.\n",
    "\n",
    "def myfunc(x):\n",
    "    return slope*x+intercept\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 200\n",
    "training_output = []\n",
    "training_input = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    training_input.append(np.array([x]))\n",
    "    training_output.append(np.array([t]))\n",
    "\n",
    "# Create a testing dataset\n",
    "n_samples = 50\n",
    "test_input = []\n",
    "test_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx] + np.random.normal(scale=0.1)\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    test_input.append(np.array([x]))\n",
    "    test_output.append(np.array([t]))\n",
    "\n",
    "# Create a perfect dataset\n",
    "n_samples = 100\n",
    "perfect_input = []\n",
    "perfect_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x)\n",
    "    perfect_input.append(np.array([x]))\n",
    "    perfect_output.append(np.array([t]))\n",
    "    \n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]\n",
    "perfect = [np.array(perfect_input), np.array(perfect_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = Network([1, 10, 1], type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.35753196157541495\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE = 0.014212274100459513\n"
     ]
    }
   ],
   "source": [
    "# On training dataset\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('Training MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE = 0.017380873656898315\n"
     ]
    }
   ],
   "source": [
    "# On test dataset\n",
    "mse = net.Evaluate(test[0], test[1])\n",
    "print('Test MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate our model and the TRUE solution (since we know it)\n",
    "s = np.linspace(-1, 1, 200)\n",
    "y = net.FeedForward(np.array([s]).T)\n",
    "p = [myfunc(x) for x in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEKCAYAAADuEgmxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJztnXuYVNWV9t/VTTd0KQoUSFCoak2YJGpMND3G6HwTLzFR841GYxINKt6GEScZM5qL2pPEaMxFYxxHjYpoVIoxXqJRRx1UFKOJN7wrBiECLciH0NxpaLqr1vfHOdVdXX0u+1SdS1Xz/p5nP1V1zj77rNrVvddZe+29lqgqCCGEEBMakhaAEEJI/UClQQghxBgqDUIIIcZQaRBCCDGGSoMQQogxVBqEEEKModIghBBiDJUGIYQQY6g0CCGEGDMsaQHCZuzYsdra2pq0GIQQUle8/PLLa1R1nF+9Iac0WltbMX/+/KTFIISQukJElpnU4/QUIYQQYxJVGiJylIgsFJHFInKhR70TRURFpC1O+QghhAwkMaUhIo0ArgdwNIC9AZwsIns71BsJ4N8AvBCvhIQQQspJ0tI4EMBiVX1PVbcD+D2A4xzqXQbgCgDb4hSOEELIYJJUGnsAeL/k83L7WB8isj+ASar6P3EKRgghxJkklYY4HOvLCCUiDQCuBnCBb0Mi00RkvojMX716dYgi1j6zZwOtrUBDg/U6e3bSEhFChjJJKo3lACaVfJ4I4IOSzyMB7AtgnogsBXAQgAednOGqOkNV21S1bdw432XGQ4bZs4Fp04BlywBV63XaNCoOQkh0JKk0XgIwWUT2FJFmACcBeLB4UlU3qOpYVW1V1VYAzwM4VlW5CcOmvR3o6hp4rKvLOk4IIVGQmNJQ1V4A3wYwB8A7AO5W1bdF5FIROTYpuUyphWmhjo5gxwkhpFoS3RGuqo8AeKTs2I9d6h4ah0wmFKeFik/5xWkhAJgyJT45Mhnr3k7HCSEkCrgjPABF6+KUU5ynhc47L155Lr8cSKUGHkulrOOEEBIFVBo+FBWFCHDqqc5P9kU6O+OdppoyBZgxA8hmLfmyWetznNYOIWTHQlTVv1Yd0dbWpmEFLCyfhjIhmwWWLg3l9oQQEhsi8rKq+oZqoqXhgdPqJD+COKFrwZlOCCFBGHKh0cOkklVIpk7oWnGmE0JIEGhpeBB0FVIQJzT3WBBC6hEqDQ+cVieJHfwkmwWmT6/cCc09FoSQeoTTUx4UFUB7uzWYZzKWIglj+oh7LAgh9QgtDR+mTLFWQxUK1mtY/gbusSCE1CNUGiXEsZqpeI9TTwVaWoB0mnssCCH1A6enbOJYzVR+j85Oy7qYNYvKghBSH3Bzn83YsdYgXk6Ym/VaW539GNwQSAhJGm7uC8Ds2c4KAwh3NRNXTBFC6h0qDXjvjQhzNZNbW1wxRQipF6g04P2kH+ZqJq6YIoTUO1Qa8H7Sb28PbxUVo9ISQuodKg04WwBFws67HdW+D0IIiQMqDQy0AJxgTChCCLGg0rApWgDF2FLlLFvGEOaEEEKlUcL2/HbsMTHvel41/OkqQgipJ6g0Srj9tdux6qCzMaKl4Fmvq8vKE06rgxCyo0GlUcKhrYfioukZzLxZkE4DgNrFmSitDmb1I4TUIlQaJUxOT8ZPD/spAMHWrQpA7OJOFE7yYoyqZcs4JUYIqS0Ye8oBtxhRbohYS2jDgjGqCCFxw9hTVeC+Q9xZwYYdBoQxqgghtUqiSkNEjhKRhSKyWEQudDh/vogsEJE3RGSuiLjspAgXNyUwakwhUBiQSv0S1cSooi+EEBIpqppIAdAI4G8A9gLQDOB1AHuX1TkMQMp+Px3AXX7tfvazn9VqyeVUUylVy6NglVTKOp7LqWYyBYUUdKdxqzWXs67Z1rPNuI1q7h/FdYQQAmC+mozdJpWiKAA+D2BOyeeLAFzkUX9/AH/2azcMpaFqDbTZrKqI9eo08Pbme1VVdXHnYh17xVj930X/23cunR44eBdLNhve/cvJZqu7JyFkx8VUaSSZuW8PAO+XfF4O4HMe9c8C8GikEpUwZYp/XKjGhsa+94e1Hob9xu8HALhu5lp0do6G08orU7+Eyf1N26YvhBASFkkqDae1rI6eZhE5BUAbgC+4nJ8GYBoAZBJITvHRMR/F3V+/u+/zDy7qhdtS3SjFy2ScV10xXwchJCySdIQvBzCp5PNEAB+UVxKRLwJoB3CsqnY7NaSqM1S1TVXbxo0bF4mwQdjW6S5DlLkzmK+DEBI1SSqNlwBMFpE9RaQZwEkAHiytICL7A7gJlsL4MAEZKyKTcd8QGGZ+jnLqLV8HV3oRUn8kpjRUtRfAtwHMAfAOgLtV9W0RuVREjrWrXQlgZwD3iMhrIvKgS3M1w+zZwObN7ueXLQPOOKsHt83aHsn9g+TrSHLQ5q53QuoT7ggPkeJA2NXlXzeTUSxb5h2iJEqcZE2l4rNMuOudkNqCO8IToL3dTGEAwPvvC/KFPA67/TDk3shFK5gDTrI6xdGKyhrhSi9C6hMqjRAJMuBlMsDarWsxrGEYmhubAQA9+R5s7dkaiWzlg79bbK3S7+A2hXTuudUrkmp2vRNCEsRkM0c9lbA291WC2+Y6EfXcpV0oFFRV9ab5N+lHfv0R7VjfEapcTjvFy2Vy2ghY6fepVCbuXickOWC4uY+WRoi4LXk95xzvFU1i55hd+ZfDsOWKt5EdPRGtrcDl1y/Ftt5tVcvlNBWlOji1bfnyXDfLqdwNVjqtZTqdVW8rvQghNiaapZ5KkpaGamXhP4rXlT95AwVtHrmh6qdvJ2uh1LJwk9XN0nAr6bRqczOtB0LqERhaGlw9VSN4+RlSKeDq6zYjv+8snLn/mRg+bLhxu7NnA6eeOtg6APxXKjmtsBJxbssLrogipPbh6qk6w8uJ3tUFXHSx4txHzsW7ne8Gare93XmQF/HfKe40hXTOOYOn4PzgiihChg60NGoEv2yBIorXVr7ZFxTx58/8HLvttBvOPuBsz3YbGtwtg0p/+tmzLWVkmt2QlgYhtQ8tjRrDz0Hs5EQvJZORPoVR0AKeeO8JPPf+c33nC+qcb9ZtCWu2inRWxV3nJm0w9hUhQwsqjVK6u8135wXAJGRGcSoonR58ffnA2yANmHvaXFx3zHUAgMVrF+Pj1318gBIpEmUQQ6e2m5qs78AVUYQMTag0Snn4YWDMGODII4ErrwRef91oDsfPinDbfX3KKf31i1M+a9dag67fwCsiaGlqAQBs6t6E3UfujtZRrQCANV1r0JPvARDt0lantn/3O2DNGrPYV3HC4IiEhITJEqt6KlUtuX3zTdXzz1fdd9/+NaPjx6uuXGmd7+kZdInJJjW3jXTF0tQU7lLV439/vB5w0wF9mwbDoNKlxLUANxIS4g9qPd1rVCW0fRorVqjedpvq9OmqxcH39NNV99tP9fvfV338cdWtW41SrAbd7+CWptV04H5o4UN6yyu39H1+dNGj2pMfrPBMqfdBl2lwCfGHSiMKbrhB9bDDLNMAUB0xQgUF11AbRZw37vkXvzZMBu4Xl7+ouAR6w0s3VPy1TQfdWrVG3Cy90v4lZEeHSiNKNm1Sffhh1fPO0+yo9c4D6qT8gEuKA2qllkalT8uFQkH/+M4fdcv2LaqqOm/JPJ31+iztzfcaf12TQddPqSWpUGhpEOIPlUZMOA6W2Ky5pqmqX/mK6jPP+NZ3KqZ+kfKB229gPvW+U7X1P1t1e+924+9YzRRcUQ63gIleCiQsRVPv02uExAGVRowMGNwm5TX3g9dUv/Md1b32Up0zx6r08suq3/ue6lNPae72Hk+ro7Fx4ICWy1nHvAZu04ExX8jr0nVLVVW1N9+rx//+eH100aO+369SZ39xwA+iIIN8H1NqdeqMkFqBSqMWKBT6neg339zvC9llF9WvfU1z0+ZpKlXwHBi9LJPSum4Dc2Oj+0DZsb5D97l+H73rrbtUVbUn36P5Qr7vvqWD7PTplQU3LF4TZCrOrz1CSPhQadQiGzeq3n+/6tlnq+6+u+rIkZq7bbs9sBY0u/t248G43BoxGZjdLI+iorjxpRt1n+v30Rtu2RD4Kd/LMjDx5ZQ7pem8JiReqDRqnUJBdcmS/s/FvSEf/7jqD36g+uyzqr29nsqg9Mk/nfYfmP2e1B9a+JCeet+pmsk4rwjze8p3mwIy8ePQ0iAkWUyVBgMW1grLlgEPPQQ88AAwbx7Q2wucdhpan77dMTBgeYjypibr2Pbt3rcRsXZre+EW5NDkWjdKgxyWy55KDd6l7hSW3akeISQcGLCw3shmgW9/G3j8cWD1auDOO4GzzrLiO7UMHKlFdNCg3tMDjBzpH0TQKwd3MdSG23PE2AldqPQhoxjkUBWYNcs/rAkz+xFSm9DSqANmX7Ma7T9qRMemUcigA8uQBSCD6hUtAbcw6yLWgO008Do92ZfSNLwHPV+ZigU3/gifHPfJqr4PIaT2oKUxRJg9G2i/ehw6No9BJiO4/HIgu+sGx7qq6pmXQ9X9Sd0pqGKRbBa4ZWYDnvz1P/cpjGuevwYPv/twwG9DCKl3qDRqmEEh1TsE0y5vxTHfGuWSe0MsnwGcrUevqSu37Hoi1rTSqac04rA9DwMA9BZ6MeOVGbjvnfs8ZWdUWUKGHokqDRE5SkQWishiEbnQ4fxwEbnLPv+CiLTGL2VyuIVUf+SR/vl+JxQCwUA/SCqlnjk03HwdTseHNQzDa//yGq768lUArHweh9x6CN768C0AZvlDwoLKiZB4SUxpiEgjgOsBHA1gbwAni8jeZdXOArBOVT8G4GoAv4pXymRxe/rv6Oh3LMtg1wYAS3FYTmRFFsswI/VdTHnlAuAvf3FcAuWWOXDzZueBuKmxCaNGjAIArNi4Auu2rkO6xcogddHFBUdl197u9k0rI07lRAixMVmXG0UB8HkAc0o+XwTgorI6cwB83n4/DMAa2M57t5LUPo0owlRUG/NJVVU3b1a95RbVY47p35E+aZLqY48NkjudVt1pp8FtmYTvKM3dAcnHsjGPezkICQ8Y7tNIcnpqDwDvl3xebh9zrKOqvQA2AHBIiJosUT3xuqVT3by5fzrmmGN80rnutBNw5plWVsLVq63lU5/+NJDJWHKfle+Tu7PT2RluYiWIbfKoKsaM3+JYRzXcKSQvS4wQEhEmmiWKAuDrAGaWfD4VwLVldd4GMLHk898ApB3amgZgPoD5mUwmXPVrQJRPvOWWgFOGP7+4UEHlrtZK8NsBHlaEWVoahIQH6sDSWA5gUsnniQA+cKsjIsMA7ApgbXlDqjpDVdtUtW3cuHERietOlE+8Rd9FoQDsvPPgHd9Fx3ixTpC83EHk89oUWE7pxjw4rOQKy7/hZIkNsLJigI54sqORpNJ4CcBkEdlTRJoBnATgwbI6DwKYar8/EcCTtkasKYKsPKqGsJWTm3zlS3ZTshWX//39wOLFxm33O+qdPfUdHdX/jEXllC6ZsGxpqbpZY+iIJzsiiSkNtXwU34bl7H4HwN2q+raIXCoix9rVbgGQFpHFAM4HMGhZbi0Q1xNv2MrJTe5zppesvBq9ETM+9itMufcEYPJk4MADgblzjZ+w3WUW5At5nPXAWXhxxYuVfQGbrVv733d2xjdwuy2JDnuVGCE1hckcVj2VobR6yukeYWegM5Z72TLVK69U3X9/zf3HAmM5vGReuGahjr9yvN791t2qOnAFlilx+jXK+yoM/w8htQIYGn1oEpVyCtJu1i10+rgtqt3dgdresn3LgHweR95xpG7YtsFY7rjybnilrKUjngwFTJUGw4jUGaWO8SBOby+Czs13vO/ip1g9AvjEJwZtHvSSOdWUQoNYf4bNjc1INaUwsnkkAGD1ltW+02Bx+ZOcpqJUB2+urGRaks50UleYaJZ6KkPd0oiCoFM8rvV367LS2qqqFgqa2+dyze6y1spKmCkEsoo2d2/WkSf9izYN7x50n3TaO8GT15Rd+RLmdNrMujJNhhXU8otiypGQSgCnp4gpQad4TAa63M1bNNW4dWCdpm7NXbPGSKYt27foqPHrjfZ6mE6tVbN/JCrfCfeakFqBSoMYU8nA5TdQu7Y5drNVYf16q3i055f3PGj6WZOUuG5tRmURMBc6qRWoNIgxUQyI7oOhvULq5z9XHTFC9aSTNPf9VzWVKgy6v98g39eW4XcyKV6DdVLxxQiJA1OlQUc4iSS1qtf+DABW0KyzzsLsh0Zi6pX7oqtroEe5qwtYuxZobna/x05jO13PeSWV8sLLgV7pIgQvR3ct7GonJBAmmqWeCi0Nb+LYT1K8j6/fI6eDLIzy0tTkHHlXpKBAQbNZ1atuWqkn3HWCLlm3pK9tv6mtoD6NqPshjt+EEC/A6SlSTtwrdSr1ezj5QXK3dvfVL1cIw0f06K4nT9eVm1aqqmpPvse17XS6stVTlcLpJ1IvUGmQQXgNpElgag0I8qq77KI6Z47rd8hk+v0bJ9x1gh56/oxIFaSpdUBHN6kXTJUGfRo7EG6BDTs7zTaUhb0JzXQDXma3buD444H99nP9Du+/35/PY99x++L/fm1jiZ9GQ/HTFAmyGTKuzYeExIaJZqmnQkvDHa/pIJPlq1HEvfJb4VR+D9fpnkn5vjZLLYD/+M8FKpeIPrTwocoFLSPIlBM375F6AZyeIuXkcu6Ds990SVRz8+WDvF9CKcdBGJs1N/knjudaWgp6Qvu9umX7FlVVnb9ivr6z+p2qZK5kM2QQRzcd4yQJqDSII257H/wG/1qamx8wqGYKmvvRO6qPPWak2L7wuy/ox/7rY31BEish6kyNtExIEoSqNAAcYnKsFgqVhjeVDkr1sArI3bFe0NzvrOi7H27+UF9Y/oKqqvbme/X7j31fF3UuCnSf6dMH38ukD00siHroZzI0CVtpvGJyrBYKlYY/lUx/1MITcDVLeAWFPt9HLmettHp15auaujyl97x9TyAZnEKkT58e/Dqn/qsli47sWISiNAB8HsAFAN6HlTmvWC4B8LrJDeIuVBrR4TZoJ5WAqjjAFu9pGjokJV2a+9b/qK5apas2r+qbqrr55Zv19D+erl3bu1zlqNQSML2OlgZJClOl4bfkthnAzgCGARhZUjbCytlNdiCcwmjElSfbLZ8F0H9PwFpW60eXtqD9v/cB9tgDu02ZhoYn5gIAOrs60bGhAyOGjbDq9XQNWma8bJlzm3552k3zuzOsCKl5TDQLgKxJvVootDTiJa4nY5ONgMV7muw0Fymofu97quPHW6+qqr29WnjzTVVV3dS9SXc9ebo2jdjuaN1EZWmocvUUSQaE7NN4CsCT5cXk2rgLlUa8RL38tIiZIui/h980Vd9gvX276gY7vexjj1knDzxQ1/72Kh05bo3rfSpxgiftEyLEC1OlYboj/HsAvm+XHwF4DcD8UE0eUpcE2fFczVSW07SN2z1Lo/YCPilZm5qAXXax3h9wAHD11cDWrRh97gXYvHq0431Ug0cELsqUTvcfa2nxvoaQmsREszgVAE9Xem2UhZZGvAR5gq52KqtopQR92g9s3RQKqi+9pNmdnS2NbFb1rrfu0o71HWaCl8jh1leckiJJg5Cnp8aUlLEAvgxgocm1cRcqjfhJInhflIPsQOVUlhwKm3Xmpy7TkZe26D/fd0agdr0CRnLqiiRN2EpjCYD37NdFAB4D8A8m18ZdqDRql3pYTuq5tHePHs2dcK9qNqtLd4Wu2GMX1Vtv1XfXvKvffvjb+uHmDz3bDprjI4p+oUVD3DBVGkY+DVXdU1X3sl8nq+qXVPXZUOfJyJCnHpaTui3tzWaBpcuHYcofvga89x6y983F7of+E5DJ4NmOZzHrtdtRmHETsGZN8UFrEEEj25Yvx602ynBcy6PJEMdEswAYAWtT330A/gDg3wGMMLnWpb0xAB6HZbU8DmC0Q53PAHgOwNsA3gDwTZO2aWnUNiZPukk8DZdOSXk9+bvJtOE3v9AcTtYsliqQ113HfKi5O3oH3cNpGsokHlgYq6/qwdIjyYGQp6fuBnALgMPsMgPAPSbXurR3BYAL7fcXAviVQ52/AzDZfr87gJUARvm1TaVR3ySxNNVkia6f4z2XU02NyA+sI1t0+jl5nTip1zOKr8l3DmPAZ4gS4kXYSmNQyBCnY6YFwEIAE+z3E2DgVAfwelGJeBUqjfqmmsExiIVSWrexMZjCcJLJ1Uopd6S3FDytK6BfntLvEMaAT0uDeBG20rgNwEElnz8H4Lcm17q0t77s8zqf+gcCeAdAg8v5abD2jczPZDKhdyaJj0oHxyAWimmMquKA6qVM/OR2KhN3/lD12Wetpb2G38FvCst02o+rtIgbYSuNdwAUACy1S8H2NbwJ4A2Xa54A8JZDOS6I0ihaIqVKy6vQ0qhvog4I6FXX7VqTtk3btEpeC4Dmzv3zgIHeTTGk06pNTYOPNzebT28V4eop4kbYSiPrVUzaKGvPaHoKwC4AXgHwddO2qTTqm0qfhoNYKCZWQek9TWTyWqpbXsZP6NLctHmaaimUnSv/7F3SaevenHYiYRC20phlcsy0ALiyzBF+hUOdZgBzAXw3SNtUGvVPJU/DYVgajY3eaWaDrvqaPt1d2QSzTLwVIh3cJAzCVhqvlH0eBmCBybUu7aVthbDIfh1jH28DMNN+fwqAHlhxrorlM35tU2nsmFTr04hqbt9N2XhlGSyXK512tkCCTJ8R4kcoSgPARQA2AeiFlUNjk106AfzC5AZxFyqNHZdKV08lMbfvau3stE4zE3tVpKCZTEFzv12vueFnaKpxW1XTZ/VC0r/LjkzYlkZNKginQqVB6gG3gf62O7arqpXP46PXfFRzT1+reu65mms5S7NYooK8Znddp7kbNgxqr9SRnk7X34A7lJRfPWKqNExDoz8qIv9YXgyvJYSUURq+vTTE+tRTmwAAG7s34tMf+TQ+ulcbcP31+KcPLsdrs55A4cijsHTzWKC72w4pomjNFPDnPwNbt/a339nZHyKk2vAjQDht+OEUwqWryzpOaggTzQLgoZLyOIANYBImQlQ1nimVi5+4WMf8aoyu7Vqrud+uH7xSy2XlVRgRdOOyAOjQTxaEOT016CJgEoA7K7k26kKlQeIkrgH11ZWv6lV/uUpVw1l55eYkd1KAcTna6dBPlqiVhgB4s5Jroy5UGiQopQNlOm0VU6shiYGuPMeHd3Gu6/T07qYA3doO2wKgTyNZTJWGkU9DRK4Vkf+yy3UAnoUVC4qQuqY8XHhnp1VUzUKHl4cv9zseBpmMOB4X0QGfU829SGONSxuDj7n5FBob3eTwFdUVJx+Jm5/HL5UuiRdTR/gCAO/C2sn9PIAfqOopkUlFSEw4DZSl+Dlig+RIDwvHfOlNW3DaWVsHDbjfOGojBGXKpEVxzDGDB203RZfPO+dBcWrDBK+8HlOmAEuXAoWC9UqFUYN4mSGwNvFdAWANrHAer9rvrwDQZGLKxF04PUWCYBJSxGsaJqkplXLfw7U3d/adO/uBs/X6F693Dm2CvB5xRCFwXo8gu939qGRKj/s3ogchbe67GsBMACNLju0CK5/GNSY3iLtQaZAgmDiV/fwTtTSgdfd261G5o/Sn837qGS7FcaXVqF5jReDVb6V94NQ3QVdJ0dcRD2EpjUUAxOF4I4BFJjeIu1BpkCD4hUnvyw+eDXeQilrR5Av5wDnJBXnNTZ1jJJdf26mUuzVikqmwFK6qigdTpeHn0yg2Vn4wD2DQcULqjXLnazptFcD6XPzrDzOfdhy5uhukwdWv0tjo/K+b2WUDppw3zvIpPP8ilp5wPqbs96ZzXR+fTVeX1a9OjnVgsI9ExOoHJ99IEosNiAdeGgXAHwGc5nD8FAAPmmiluAstDRIG1T7delkScT05O/o0mrt0/KH3+k/33HBDfxKPtjbV3/5Wdd06z7aNLRoZuP/DL5UuLY14QEjTU3sAeAHAPABXAfg1gKcBvAhgD5MbxF2oNEgYVLM72W8OPs6dz+XKq/3qt/XRRY9qLqeayRQUUtBJmbxzNOAbNmh29AYr3hWWaG7Uv6rm84PaDqo0TJJXldahTyMeQlEafZWAwwF8B8C/ATjC5JqkCpUGCYNqnm79rq2VJ+dH3n1EcQn0vgX3DTrnOFA391gDdT6vevjhqj/5ieqSJYGsjvLB3lSB1tJig6FKqEqjngqVBgmDap5u/QbCWnlyLhQK+syyZzRfsKyH/37jv/X6F6/XfCHvqdhyN27U7Ij/12+B7H255s55RrOT8p4Kw2m3fa0oUEKlQUjVVPp0azrlYtK2U71K5fK77pv3fFMPvuVgLRQKnqujBvkgZIvmcLLq/fd7XuekKKvZ70HChUqDkIQIy5JwaqepSbW5OXjbXjL1K5OCTpzUq7mc6qSMt9UwSCGO36ra3R3Yx+G0cbAWFIaXTLUobxhQaRCSIGEMLEEGYL/pHLe23EKnf+vMddrQ3FVmYbgHShww9Ta8t+y8/3WmVJKrPQxlXapgh6plRKVBSJ0TZHOe3+AbdKOfNdgW+gbfXXZb6zn4O0+9FbSxwdtiCeK7MBmwwxjUvaYXh7IPxlRpiFV36NDW1qbz589PWgxCqqa11drwZkI2awX4C6MtwNpsVyj0f37sb4/hG4cciA2rRjnWnTXLObhgQ4M1rDqRatyGGee9jSm/3A9oavKVye07lH53kzp+uMksdnBht3Ol/VWPiMjLqtrmV880yi0hJGacotk2NQHNzQOPpVJW3aBtpVL9u9/LKd/x/aWPfgnXXzXKcSf3Oee4R6N13ZUuecxoOQ9TftMG7L478Mwz3l8AZjvDw9g97hW5OImoxjWHiTlST4XTU2QoEfXqKTdnu1siquKmQJGC7jHR2rfx7pp39fbXbtc7ZvUatd83XbR9u+pDD6medJLqmjXWDe6+W/VHP1JdsGCQ/CZTQ2FMH9GnQZ8GIcSD8syFQVdnXfzExdp04lRtaSm4DrTGSu7881UbGqwGPv1p1V/8QnXJkj454/BplPcJV09RaRAyZAh7AKvkST1fyOuEid3hOYhXrlS95hrVgw6yGjnooL5TuZu3RL56akfFVGnQEU5InVKMlltQNDfqAAAWmklEQVQaSTaVqi5FqpcT2MvR63Vdb76ABmnok7m93fIxZDKWr8VT1iVLrPy7bW3A+vWW/+Pgg4GTTwZOOAEYPTrYFySu1LQjXETGiMjjIrLIfnX95UVkFxFZYecmJ4TYuOX0Pu+8ytKwApU7et3O77b7Nnzqhk9h4ZqFlYWE33NPS2EAVt7ZCy6wlkGdfTYwfjxw7LHAm87h20k0JLV66kIAc1V1MoC59mc3LoMVWZcQUoLbiqDOzspzdbitsqp0ddZZ3/sbJuw8AZldM65KzisH+wDSaeCyy4BFi4CXXgK+8x3glVeA4cMBALMvXYzW3brQ0KCBlSUJgMkcVtgFwEIAE+z3EwAsdKn3WQC/B3A6gOtM2qZPg+wohLljvJSoYlu57SivKiS8Hao9l1NNNW4b6AAf3qO5W7uraHzHArXsCAewvuzzOoc6DbDyeEzyUxoApgGYD2B+JpMJuSsJqU2ChCSPIleHn2zlCiSTcVYamUyh6vu5OvCbVvRX6u2t+j5hUKuOelOlEdn0lIg8ISJvOZTjDJs4F8Ajqvq+X0VVnaGqbaraNm7cuOoEJ6ROKE9Vm82ab9aLEjffxVe+IoOmsNC0Bd/8t9cxe/ZAP8y55wbzy7hu6uudYL3p6QH22gv4xjeAe+4Btmyp6jtWShypfiPHRLOEXWAwPQVgNoAOAEsBrAGwEcAv/drm9BTZkXFM8SpWCPK48MzFUfKUnckU9LwrXtBZubyvxeS318L3nhN7VVDQbEOHFca9pUX1xBNV588fkNejsXHgdUXCsg5qOXYVanx66koAF9rvLwRwhU/900GfBiFGTJ/un3c7SoKmszX1zXgNrG6b+hzzdQzv1dwXb1UdP15z7W+7KqwodoHHmeo3KLWuNNKwVk0tsl/H2MfbAMx0qE+lQXY4okwCFSVB728agddvYHXqLzdZGhtVc3fkNZ12j9xblDnM/kz6t/GippVGlIVKgyRJWNMYUaabDSpH0O8TVHZTS8PJYe6/YsusbXdF5Z7FsNL+rNXYVVQahMRMmANCNU+klVxrGszQ9PsESZZUHIC9Bu9hw7v1Y2f/WLt7uwdc75eNsBqF0WdpuKz6qtQ6qPfVU4kP8mEXKg2SFGFOPVTzdGs62HsN2qmUFbwwqqkUN4d9sf3p0wcOrOf8/E96wZwL+q7/6+q/ui7hdcpGWEnpU0B39GqqafvAc7JFc+f8qfqOqCGoNAiJmTCnMapVQH5Ps0H2eHh9nyT8Lu9veF+bL2tWeKSfDcPCcF49VdDsmI2a+8yVqvfea518913VU05Rvesu1fXrzTqgBqHSICRmwrQ0op77rmbqpvh9kvK7bOvZpje8dIPuMbEndGVRUR8//HC/WTZsmOrhh6tefXXdKRAqDUJiJuyBPsq5bxMHsdM0T+n3icvv4tYPbv3tNq2WTnv7UNLpKvq4t1f1mWdUf/hD1b33tpZnrV1rnXv6adU//Um1p6fCxuOBSoOQBKhVJ2c5fpaGSQKluPwuXvVyOdVJmbw1bWTLd/zF9+jwll7Pa0oVS1XKwo0PPuh/f/jh1o3GjKnpaSwqDUKIK36OaJNBNGq/SyX3WNu1Vif8eoJ+rf0PgayTSJX7+vVWGttTT+3XVkcc0X9+0SLVQvXxt6qFSoMQ4km1VlEcA3Al1kzX9i7dsn2LqqrOWzJPj7j9CO1Y36GqyW6uy+Ws5bsiBc2O32r109q1VnrbCRNUzzjDUi7Faa2YMVUaSeXTIIQkzJQpVj6jQsF6DZrtzylgYjVZA52oJClUS1MLUk1WZMQ1XWuwdutajE2NBQB0dKjjNW4BD8OiL1Bhh0BVsGzVCCtQ4f0twMyZwD/8A3D//VZAxbFjgTvvtC7s6fFOmZgAVBqEkIqpVvH4UWlSqCLbXv0aOn/+MnYa3oJsVtGQWu9YL+oowK4JqC4dAZxxBnD33cDq1cCf/wxcfDFw4IFWpXvuASZMAE47zVIka9ZEK6gBw5IWgBBC3CgqoUB5xW36c6gLAKCjQ9DQOBLDmvLo7WnsqxdECVWKa+j20uPDhln5zw8+uP9YNgt88YvAI48As2ZZJt0BBwBPPQWMHBmpzG7Q0iCE1DTl1gxglmvD6em+kB+GXXdpRDYLQBTYdSku+PlfQ7WQynODzJ5dee51HHKI1cCqVcALLwCXXAJMntyvMM48Ezj6aOA3v7Fypavz9FuomDg+6qnQEU7I0CWI893Pib5x20a98aUbNV+wUsY+/O7D+tKKlyKRzzFEe5ncFS1M+PGPVT/xif5GV6zwv8YFcPUUIWSoEWT1U5C6hUJB97thP/27f76kqhVlpgmonMKUVLUSraPDWnlVBaZKQ6y6Q4e2tjadP39+0mIQQiKgocF5BkZk8CKjfp9G/7FUyn2F18zbu/Cd6SOwbWv/rP2IlgJm3txgPH0VRL5SWlut1K/lZLP9U3JRIyIvq2qbXz36NAghdUMQ30DQJcE/+0lqgMIAgG1bG9Debs3IhC1fKUaO8hqBSoMQUjcEXYIbZEmw18Dd/mQ7vn7P15Ev5EOVr0jFjvIEoNIghNQNbtYDYLaiygu3AVoVuGHKD7H2haPQ2GAt1V21eVUg+fymt6rdjxIrJo6Peip0hBOyYxFWOBO/HCPFNjvWd+iIn43Qm+bfFPr3SDLYJegIJ4TsCITpRJ4929rf4dResc3X/roeV/3lKpx9wNnIjspi6fql6Orpwt7j9g4qek1BRzghZIcgTCdy0Qci4t7mqBGjcNnhlyE7KgsAuPTpS/G5mZ/Dpu5NwW/ogdMmwVqASoMQUtdE4UQO0uYVR16B6c3P4FMfH4mGBmDMRzbiyhtXGN/LSTn0BThcZk2OLVtmfa4JxWEyh1VPhT4NQnYsogjR7ubfcErY5FS3aXi30f3dZN95Z2e/SpQh3MHQ6ISQHYEoQrQX20ynBx7v7Bz8xO8U46qnuxnt7cDTS5/GafefhjVdztFp3aLfbt7sLFct7Nug0iCE1D1RhGifMgXYeefBx7u6rMG+iJdPZWHnQjy3/Dns1LQTAKAn3zOoThBqYd9GIkpDRMaIyOMissh+He1SLyMij4nIOyKyQERa45WUELIjY+Jk9/J/TPvsNCw4dwFamlqgqjjk1kPw46d+7HutG6X7NpJylCdlaVwIYK6qTgYw1/7sxB0ArlTVTwI4EMCHMclHCEmYWlg9ZOIQ99uY19TYBADY1rsNh0w6BJ8Y+wkAQG+hF99tXz3oWreVW+l0vwWVqKPcxPERdgGwEMAE+/0EAAsd6uwN4NmgbdMRTkj9E0f+8TDlqGRj3h2v3aHDLh2mP7vuvQHXOoVRb2qynPDFOul0+I5y1HJodADryz6vc6jzVQD/A+A+AK8CuBJAo0t70wDMBzA/k8lU3muEkJogSFjzqIlqp/byDcv1Z0//rC+fx1NLntKl65YOumc6rdrc7NwfbrlCKsFUaUS2I1xEngDwEYdT7QBuV9VRJXXXqeoAv4aInAjgFgD7A+gAcBeAR1T1Fq/7ckc4IfVPpSHG65WCFjD52snI7prFk1OfHHDObce7E9WEUk98R7iqflFV93UoDwBYJSITbEEnwNlXsRzAq6r6nqr2AvgjgAOikpcQUjvUU9TXIm6b9Ez8Mg3SgHlT5+Hao68FAGzq3oTz55yPFRtXGK+wiivAYVKO8AcBTLXfTwXwgEOdlwCMFpFx9ufDASyIQTZCSMLUVdRXODumzzjDSuFt6qyetOsk7LPbPgCAZzuexXUvXocVm1a4Ksp0Oty9KcaYzGGFXQCkYa2aWmS/jrGPtwGYWVLvSABvAHgTwG0Amv3apiOckKFB0lFfg+Dmg6kk9WuRVZtXqap1rml49yDfRWlbYYCkfRpJQZ8GISRu3HwwbqRSzmloAWvjYEeHNRV3+eWW9XD0D2bhmduOxpbVYyEy8F5eKWyDYOrToNIghJAqCeKsbmwE8g4JANNpYOtW95zmqoo995TIcokn7ggnhJChTKmTe/NmoLl54PmmpsHHUilnhQFYca2c4lCdd551n8ZGZ4UBAB0d8T38U2kQQkhAyh3fnZ3Wazrd75j+3e+AW28d7KzOZoPdq7Oz/z5uyK7Lsa13W3VfypBhsdyFEEKGEI6RbXusAIdrygLaOvkapk0bfH1DQ2V7UEa0FHD2hcsxYtik4BdXAC0NQggJiEkgQ7c9Gm5h1ytRGOk0MPPmBlz7w88Hv7hCqDQIIXVJEgENi/d0myoq7qnwCyjoFnYdsBzlxemscsVSztatA+WKpS9M1uXWU+E+DUKGPkkENHTL5ud0f699G8W9FcW9Fl7xo/zuWcwmGEZfgPs0CCFDFbclrmEsPQ16z+J9i3sqAP99G6kU0NJiObmd2ir9DrNnWz4U0yW9bu34wSW3hJAhi4lPIa57igzOFugXI6voBDcJlVLMShh01VVUfUGlQQipO5IIaBjknk6xs8pZuzZYbnO3eFxufo+o+oJKgxBSdyQR0DDIPYsrpLysg0wmWG7z0jZLlcw118TcFyaOj3oqdIQTsmOQREDD8ntOn+4vQxxO+zD6AnSEE0JIdBSX1brFiiqv6xSIsJZgwEJCCImQJFZwRQlXTxFCSIQksYKrFqDSIISQCqjHlLRhQKVBCCEVUG8pacOCSoMQQirAbQlsrTm4w4ah0QkhpEKmTBn6SqIcWhqEEEKModIghBBiDJUGIYQQY6g0CCGEGEOlQQghdUzcGQy5eooQQuqU8vhXxbSyQHSruhKxNERkjIg8LiKL7NfRLvWuEJG3ReQdEfkvEZG4ZSWEkFqlvX1gwETA+tzeHt09k5qeuhDAXFWdDGCu/XkAInIwgEMA7AdgXwB/D+ALcQpJCCG1TBLxr5JSGscBuN1+fzuArzrUUQAjADQDGA6gCcCqWKQjhJA6IIn4V0kpjfGquhIA7Nfdyiuo6nMAngKw0i5zVPUdp8ZEZJqIzBeR+atXr45QbEIIqR2SiH8VmdIQkSdE5C2Hcpzh9R8D8EkAEwHsAeBwEflHp7qqOkNV21S1bdy4ceF9CUIIqWGSiH8V2eopVf2i2zkRWSUiE1R1pYhMAPChQ7XjATyvqpvtax4FcBCAP0UiMCGE1CFxx79KanrqQQBT7fdTATzgUKcDwBdEZJiINMFygjtOTxFCCImHpJTGLwEcKSKLABxpf4aItInITLvOvQD+BuBNAK8DeF1VH0pCWEIIIRaJbO5T1U4ARzgcnw/gbPt9HsC/xCwaIYQQDxhGhBBCiDFUGoQQQowRVU1ahlARkdUAllXRxFgAa0ISJ0woVzAoVzAoVzCGolxZVfXdszDklEa1iMh8VW1LWo5yKFcwKFcwKFcwdmS5OD1FCCHEGCoNQgghxlBpDGZG0gK4QLmCQbmCQbmCscPKRZ8GIYQQY2hpEEIIMWaHVBoi8nU7I2BBRFxXGojIUSKyUEQWi8iFJcf3FJEX7MyDd4lIc0hy+WY0FJHDROS1krJNRL5qn7tNRJaUnPtMXHLZ9fIl936w5HiS/fUZEXnO/r3fEJFvlpwLrb/c/lZKzg+3v/tiuy9aS85dZB9fKCJfrlSGCuU6X0QW2H0zV0SyJeccf88YZTtdRFaXyHB2ybmp9u++SESmll8boUxXl8jzroisLzkXWX+JyK0i8qGIvOVyXsTKbrrY/i0PKDkXbl+p6g5XYIVc/ziAeQDaXOo0wop9tResRFCvA9jbPnc3gJPs9zcCmB6SXFcAuNB+fyGAX/nUHwNgLYCU/fk2ACdG0F9GcgHY7HI8sf4C8HcAJtvvd4eVm2VUmP3l9bdSUudcADfa708CcJf9fm+7/nAAe9rtNIbUPyZyHVby9zO9KJfX7xmjbKcDuM7h2jEA3rNfR9vvR8chU1n97wC4Nab++kcABwB4y+X8MQAeBSCwooG/EFVf7ZCWhqq+o6oLfaodCGCxqr6nqtsB/B7AcSIiAA6HFVARcM88WAkmGQ1LORHAo6ra5VOvWoLK1UfS/aWq76rqIvv9B7DC8IeddMXxb8VD1nsBHGH3zXEAfq+q3aq6BMBiu71Y5FLVp0r+fp6Hlb8mDkz6zI0vA3hcVdeq6joAjwM4KgGZTgZwZwj39UVV/wTrAdGN4wDcoRbPAxglVtqJ0Ptqh1QahuwB4P2Sz8vtY2kA61W1t+x4GPhmNCzjJAz+o73cNk+vFpHhMcs1QqwMis8Xp8xQQ/0lIgfCeoL8W8nhMPrL7W/FsY7dFxtg9Y3JtZUStO2zYD2tFnH6PcPCVLav2b/PvSIyKeC1UckEexpvTwBPlhyOsr/8cJM99L5KJMptHIjIEwA+4nCqXVWd8ncMasLhmHocr1ou0zbsdiYA+BSAOSWHLwLw/2ANjDMA/BDApTHKlVHVD0RkLwBPisibADY61Euqv2YBmKqqBftwxf1V3rzDsfLvGMnfkw/GbYvIKQDaYOWtKTLo91TVvzldH5FsDwG4U1W7ReQcWJba4YbXRiVTkZMA3KtWNO4iUfaXH7H9fQ1ZpaEemQMNWQ5gUsnniQA+gBXXZZSIDLOfGIvHq5ZLzDIaFvkGgPtVtaek7ZX2224R+R2A78Uplz39A1V9T0TmAdgfwB+QcH+JyC4AHgbwH7bpXmy74v4qw+1vxanOchEZBmBXWNMNJtdWilHbIvJFWEr4C6raXTzu8nuGNQj6yqZWCoUiNwP4Vcm1h5ZdOy8OmUo4CcC/lh6IuL/8cJM99L7i9JQ7LwGYLNbKn2ZYfyQPquVdegqWPwFwzzxYCSYZDYsMmk+1B86iH+GrABxXWkQhl4iMLk7viMhYAIcAWJB0f9m/3f2w5nvvKTsXVn85/q14yHoigCftvnkQwElira7aE8BkAC9WKEdguURkfwA3AThWVT8sOe74e4Ykl6lsE0o+Hov+zJ1zAHzJlnE0gC9hoMUdmUy2XB+H5VR+ruRY1P3lx4MATrNXUR0EYIP9UBR+X0Xl7a/lAiv/+HIA3QBWAZhjH98dwCMl9Y4B8C6sp4X2kuN7wfrHXgzgHgDDQ5IrDWAugEX26xj7eBuAmSX1WgGsANBQdv2TsDIdvgUgB2DnuOQCcDD6syy+CeCsWugvAKcA6AHwWkn5TNj95fS3Amuq61j7/Qj7uy+2+2Kvkmvb7esWAjg65L91P7mesP8Hin3zoN/vGaNsvwDwti3DUwA+UXLtmXZfLgZwRlwy2Z8vAfDLsusi7S9YD4gr7b/l5bD8T+cAOMc+LwCuR3+207aSa0PtK+4IJ4QQYgynpwghhBhDpUEIIcQYKg1CCCHGUGkQQggxhkqDEEKIMVQahFSBiGyOoM1WEflW2O0SEgZUGoTUHq0AqDRITUKlQUgIiMihIjLPDqz3VxGZbe80h4gsFZFficiLdvmYffw2ETmxpI2i1fJLAP9HrLwM/x7/tyHEHSoNQsJjfwDfhZUjYy9YoSSKbFTVAwFcB+A/fdq5EMAzqvoZVb06EkkJqRAqDULC40VVXa5WFN3XYE0zFbmz5PXzcQtGSFhQaRASHt0l7/MYGEVaHd73wv4ftKeyQkmDS0iUUGkQEg/fLHktRkddCuCz9vvjADTZ7zcBGBmbZIQEYMjm0yCkxhguIi/AelA72T52M4AHRORFWFF6t9jH3wDQKyKvA7iNfg1SSzDKLSERIyJLYYWqXpO0LIRUC6enCCGEGENLgxBCiDG0NAghhBhDpUEIIcQYKg1CCCHGUGkQQggxhkqDEEKIMVQahBBCjPn/cfaPQv90JS8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x206dd521f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training data,\n",
    "# as well as out model and the true model\n",
    "plt.plot(s,y, 'r--')\n",
    "plt.plot(s,p, 'g:')\n",
    "plt.plot(training_input, training_output, 'bo')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
